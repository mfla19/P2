\chapter{Lineær algebra}
Dette kapitel er skrevet med udgangspunkt i \citep{lial}

Lineær algebra bruges til at løse lineære ligningssystemer. 
En af de mest nyttige algoritmer, til at løse sådanne ligningssystemer, er Gaussisk elimination, som vil blive beskrevet senere i dette kapitel.

\section{Matricer}
Lineære ligningssystemer kan opskrives i matricer. 
En matrix er defineret i Definition \ref{def:matricer}.

\begin{defn} [Matrix]
En matrix er en rektangulær tabel over skalarer. 
Størrelsen på en matrix er $m \times n$, hvor $m$ er antal rækker og $n$ er antal søjler. 
En matrix kaldes kvadratisk hvis $m=n$. 
Skalaren i den $i$'te række og $j$'te søjle kaldes $(i,j)$-indgangen.
\label{def:matricer}
\end{defn}

Herunder ses en matrix $\underset{m \times n}{A}$ og dens indgange, $a_{i,j}$. Den $i$'te række i A betenges $\vec{a}_i$, mens den $j$'te søjle angives $\vec{A}_j$.

\begin{align*}
\underset{m \times n}{A} = \begin{bmatrix}
	a_{1,1} & a_{1,2} & \dots & a_{1,j} & \dots & a_{1,n} \\
	a_{2,1} & \ddots  &       &         &       & \vdots \\
	\vdots  &         & \ddots &        &       & \vdots \\
	a_{i,1} &         &       & a_{i,j} &       & \vdots \\
	\vdots  &         &       &         & \ddots& \vdots \\
	a_{m,1} & \dots   & \dots & \dots   & \dots & a_{m,n} 
\end{bmatrix}
\end{align*}

Matricer kan bruges til at vise mange forskellige ting fra den virkelige verden, det kan være antal af varer solgt fra forskellige butikker, eller prisen på forskellige produkter. 

\begin{eks}
Herunder ses en $4 \times 3$-matrix, der viser antallet af solgte varer fra tre forskellige butikker. Hver række i matricen repræsenterer en bestemt vare, mens hver søjle repræsenterer en butik. Den første række viser altså hvor mange trøjer hver butik har solgt.
\begin{align*}
\begin{matrix}
	Trøjer \\
	Kjoler \\
	Bukser \\
	Jakker
\end{matrix}
\begin{bmatrix}
	24 & 35 & 43 \\
	10 & 47 & 24 \\
	33 & 25 & 32 \\
	28 & 51 & 37
\end{bmatrix}
\end{align*}
I denne matrix kan man se, at der i dens $(2,3)$-indgang står $24$, hvilket betyder, at den tredje butik har solgt $24$ kjoler. Der står i dens $(3,1)$-indgang $33$, hvilket betyder, at den første butik har solgt $33$ par bukser.
\end{eks}

\begin{defn}[Transponeret matrix]
Lad $A$ være en $m \times n$ matrix. Så er den transponerede matrix, $A^T$, en $n \times m$ matrix, hvor hver indgang $(j,i)$ i $A^T$, er den $(i,j)$'te indgang i $A$
\label{def:(transmatrix)} 
\end{defn}
Det vil sige at rækkerne i $A$, bliver til søjlerne i $A^T$, og at søjlerne i $A$ bliver til rækkerne i $A^T$.

\begin{eks}
Givet en matrix $A$ 	bestemmes nu den transponerede
\begin{align*}
A = \begin{bmatrix}
	5 & 3 & 3 \\
	1 & 2 & 5
\end{bmatrix}
\end{align*}

\begin{align*}
A^T = \begin{bmatrix}
	5 & 1  \\
	3 & 2  \\
	3 & 5
\end{bmatrix}
\end{align*}
Det ses at $A$ er en $2 \times 3$ matrix og $A^T$ er en $3 \times 2$ matrix. 
\end{eks}


En bestemt type af matrix er en identitetsmatrix, der betegnes $I_n$. 

\begin{defn} [Identitetsmatrix]
For hvert positivt heltal $n$, er $n \times n$ identitetsmatricen, $I_n$, den $n \times n$ matrix, hvor hver søjle er standardvektorerne $\vec{e_1}$, $\vec{e_2}$, $\dots$, $\vec{e_n}$ i $\mathds{R}^n$.\\

alternativ definition:\\
For hvert positivt heltal $n$, er $n \times n$ identitetsmatricen, $I_n$, defineret som\\ $I_n = \rvect{\vec{e_1} & \vec{e_2} & \dots &  \vec{e_n}}$, hvor $\vec{e_1}$, $\vec{e_2}$, $\dots$, $\vec{e_n}$ er standardvektorerne i $\mathds{R}^n$
\label{def:imatrix}
\end{defn}

En identitetsmatrix med $3$ rækker og $3$ søjler, ser således ud:
\begin{align*}
I_3 = \begin{bmatrix}
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & 0 & 1 
\end{bmatrix}
\end{align*}

En anden bestemt type matrix er en delmatrix, som er en form for undermatrix. 
\begin{defn} [Delmatrix]
En matrix $A'$ er en delmatrix af $A$, hvis $A'$ kan dannes ved at fjerne hele søjler og/eller hele rækker fra $A$.
\label{delmatrix}
\end{defn}

Givet en matrix $A$ kan der dannes delmatricen $A'$.

\begin{align*}
A &= \begin{bmatrix}
	1 & 2 & 3 \\
	4 & 5 & 6 \\
	7 & 8 & 9 
\end{bmatrix}\\
A' &= \begin{bmatrix}
	5 & 6 \\
	8 & 9
\end{bmatrix}
\end{align*}

\input{incl/main/Lin_alg/vectorer}

\section{Regneoperationer med matricer}
Givet to $m \times n$ matricer $A$ og $B$ kan der udføres forskellige regneoperationer. 
Summen af to matricer findes ved at addere en indgang i $A$ med tilsvarende indgang i $B$, så $A+B$ er en $m \times n$ matrix, hvor indgang $(i,j)$ er $a_{i,j}+b_{i,j}$. 
Det samme gør sig gældende ved substraktion. \\
Givet en $m \times n$ matrix $A$ og en skalar $c$, er produktet af skalaren og matricen, $cA$, en $m \times n$ matrix, hvor indgangene er $c$ gange den tilsvarende indgang i $A$. \\

\begin{stn}
Lad $A$, $B$ og $C$ være $m \times n$ matricer, og lad $s$ og $t$ være tilfældige skalarer. 
Så gælder følgende
\begin{enumerate}[label=(\alph*)]
\item $A + B = B + A$
\item $(A + B) + C = A + (B + C)$
\item $A + O = A$
\item $A + (-A) = O$
\item $(st) A = s (tA)$
\item $s(A + B) = sA + sB$
\item $(s+t)A = sA + tA$
\end{enumerate}
\label{stn_regn}
\end{stn}

\begin{proof} 
Lad $A$, $B$ og $C$ være $m \times n$ matricer. Lad matricen $O$ være en nulmatrix. \\
(a) Det vises, at enhver indgang i $A + B$ er tilsvarende i $B + A$. Betragt indgangene $a_{i,j}$ og $b_{i,j}$. Summen af $a_{i,j} + b_{i,j}$ er det samme som $b_{i,j} + a_{i,j}$. \\
(b) Det vises, at enhver indgang i $(A + B) + C$ er den samme som den tilsvarende indgang i $A + (B + C)$. Ligesom i (a) tages der udgangspunkt i indgang $(i,j)$. Summen af $(a_{i,j} + b_{i,j}) + c_{i,j}$ er det samme som $a_{i,j} + (b_{i,j} + c_{i,j})$. Ifølge den associative lov, er det uden betydning hvor paranteserne er placeret i et additionsudtryk. Derfor må indgangen $(i,j)$ i $(A + B) + C$ være lig indgang $(i,j)$ i $A + (B + C)$. \\
(c) For enhver indgang i $A$, $a_{i,j}$ skal denne adderes med $0$. $a_{i,j}+0=a_{i,j}$. Derfor må $A + O = A$. \\
(d) For enhver indgang i $A$, $a_{i,j}$ skal denne fratrækkes samme indgang i $A$, $a_{i,j}$. Da \\ $a_{i,j} - a_{i,j} = 0$, må $A + (-A) = O$. \\
(e) Her tages der udgangspunkt i den (i,j)-indgang af A, $a_{i,j}$. Det ses så, at udsagnet bliver til $(st)a_{i,j} = s(ta_{i,j})$ når flere tal multipliceres, er det lige meget i hvilken rækkefølge, dermed er (e) bevist. \\
(f) Hver indgang i A, $a_{i,j}$ lægges sammen med den tilsvarende indgang i B, $b_{i,j}$, dette giver venstresiden $s(a_{i,j}+b_{i,j})$. Da det er tilladt at multiplicere ind i en parantes, kan udtrykket skrives sådan, $s(a_{i,j}+b_{i,j})=sa_{i,j}+sb_{i,j}$, dermed er (f) bevist. \\
(g) På samme måde som før tages der udgangspunkt i den (i,j)-indgang af A, $a_{i,j}$. Udtrykket på venstresiden er så $(s+t)a_{i,j}$. Igen kan der multipliceres ind i parantesen og dermed fås følgende, $(s+t)a_{i,j}=sa_{i,j}+ta_{i,j}$, hvilket beviser (g).
\end{proof}

\begin{eks}
For at vise eksempler på nogle af de ovenstående regneoperationer, tages der udgangspunkt i matricerne A og B, samt skalaren s, hvor $s(A+B)$ skal findes.
\begin{align*}
A= \begin{bmatrix}
	2 & 3 & 4 \\
	5 & -2 & 1 	
\end{bmatrix},  
B= \begin{bmatrix}
	1 & 2 & -1 \\
	-3 & 4 & 0
\end{bmatrix},
s=2
\end{align*}
Først udføres regneoperation (a) fra Sætning \ref{stn_regn},
\begin{align*}
A+B= \begin{bmatrix}
	2 & 3 & 4 \\
	5 & -2 & 1 	
\end{bmatrix}  
+ \begin{bmatrix}
	1 & 2 & -1 \\
	-3 & 4 & 0
\end{bmatrix}
= \begin{bmatrix}
	3 & 5 & 3 \\
	2 & 2 & 1
\end{bmatrix}.
\end{align*}
Herfter udføres (f),
\begin{align*}
s(A+B)= 5 \times \left( \begin{bmatrix}
	2 & 3 & 4 \\
	5 & -2 & 1 	
\end{bmatrix}  
+ \begin{bmatrix}
	1 & 2 & -1 \\
	-3 & 4 & 0
\end{bmatrix} \right)
= 5 \times \begin{bmatrix}
	3 & 5 & 3 \\
	2 & 2 & 1
\end{bmatrix}
= \begin{bmatrix}
	15 & 25 & 15 \\
	10 & 10 & 5
\end{bmatrix}.
\end{align*}
\end{eks}


\subsection{Matrix produkt}
Multiplikation af to matricer gøres ikke ved af multiplicere på tilsvarende indegange i to matricer. Det at gange to matricer sammen defineres herunder. 
\begin{defn} [Matrix produkt]
Lad $A$ være en $m \times n$ matrix og $B$ være en $n \times p$ matrix. Da er en produktet $A \cdot B$ en $m \times p$ matrix, hvor indgangene er givet ved: 

$$(AB)_{i,j} = \vec{a}_i \cdot \vec{B}_j$$

hvor $\vec{a}_i$ er den $i$'te række i $A$, og $\vec{B}_j$ er den $j$'te søjle i $B$
\label{def:(matrixprodukt)}
\end{defn}
Det er vigtigt at antallet af søjler i $A$ er det samme som antallet af rækker i $B$. Er det ikke tilfældet, så er matrix produktet ikke defineret. Matrix produktet er oftest ikke kommutativt. Ligningen $AB=AB$ er derfor ikke altid gældende. 
\begin{eks}
Nu tages udgangspunkt i to matricer $A$ og $B$. 
\begin{align*}
\underset{2 \times 3}{A}= \begin{bmatrix}
	\bf{1} & \bf{3} & \bf{-2} \\
	5 & 4 & 0 	
\end{bmatrix},
\underset{3 \times 4}{B}= \begin{bmatrix}
	\bf{2} & 3 & -1 & 3 \\
	\bf{1} & 4 & 5 & 5\\
	\bf{1} & 0 & 4 & 2
\end{bmatrix}  
\end{align*}
For at beregne indgang $(1,1)$ benyttes række 1 i $A$ og søjle 1 i $B$. 
$$ab_{1,1}=1\cdot 2+3\cdot 1-2 \cdot 1 = 3$$ 
De resterende indgange beregens på samme måde. 
Matrix produkt af $A$ og $B$ er dermed givet ved:
\begin{align*}
\underset{2 \times 4}{AB}= \begin{bmatrix}
	\bf{3} & 15 & 6 & 14 \\
	14 & 31 & 15 & 35
\end{bmatrix}  
\end{align*}
\end{eks}

\input{incl/main/Lin_alg/lin_lign}

\subsection{Invers matrix}
Hvis matrix produktet af to matricer, A og B, giver identitetsmatricen, sigen B matrix at være den inverse matrix til A. 
\begin{defn}[Invers matrix]
Lad $A$ og $B$ være kvadratiske $n \times n$ matricer. Lad $AB=BA=I_n$, hvor $I_n$ er identitetsmatricen. Så er A invertibel og B er den inverse matrix til $A$. $B$ noteres da $A^{-1}$. 
\label{def(inversmatrix)}
\end{defn}
Den inverse til matrix til $A$, er en entydig matrix. Antag at der findes to inverse matricer til $A$, disse kaldes $B$ og $C$. De to matricer vil være den samme da: 
\begin{align*}
B=BI_n=B(AC)=(BA)C=I_nC=C
\end{align*}
Den inverse matrix kan bruges til at løse specifikke ligningssystemer. Lad $A$ være en $n \times n$ matrix, og $\vec{b}$ være en vektor i $\mathds{R}^n$. Betragt ligningssystemet: 
\begin{align*}
A \vec{x} &= \vec{b}\\
A^{-1} A \vec{x} &= A^{-1} \vec{b}\\
I_n \vec{x} &= A^{-1} \vec{b}.
\end{align*} 
Ved at gange den inverse matrix på begge sider af lighedstegnet, opnåes et udtryk for den entydige løsning til ligningssystemet: 
\begin{align}
\vec{x} &= A^{-1} \vec{b}.
\end{align} 

\begin{stn}
Lad $A$ være en $n \times n$ matrix. Der gælder at: 
\begin{enumerate}[label=(\alph*)]
\item $A$ er invertibel hvis og kun hvis den reducerede trappeform af $A$ er identitesmatricen, $I_n$
\item Hvis $A$ er invertibel, så vil de samme elementære rækkeoperationer som reducerer $A$ til $I_n$ føre $I_n$ over i $A^{-1}$.  
\end{enumerate}
\begin{align*}
\begin{bmatrix}
A & I_n
\end{bmatrix} \sim \dots \sim
\begin{bmatrix}
I_n & A^{-1}
\end{bmatrix}
\end{align*}
\label{stn:inversmatrix}
\end{stn}

\begin{proof}
(a) Først lad $A$ være invertibel. Betragt nu en vektor $\vec{x}$ i $\mathds{R}^n$, som opfylder $A\vec{x}=\vec{0}$. Så fåes det at ligning $(4.1)$, at $\vec{x}=A^{-1} \vec{0}=\vec{0}$. Da løsningen af  $A\vec{x}=\vec{0}$ er $\vec{0}$ må $rang(A)=n$. Det betydet at antallet af pivot-søjler er det samme som antellet af søjler, samt antallet af rækker. Den reducerede trappeform af $A$ må derfor være identitetsmatricen. 

Lad nu den reducerede trappeform af $A$ være identitesmatricen. Så må der findes en invertibel $n \times n$ matrix $P$ således at: $PA=I_n$
\begin{align*}
A=I_nA=(P^{-1}P)A=P^{-1}(PA)=P^{-1}I_n=P^{-1}
\end{align*}
Da $P$ er invertibel må $P^{-1}$ også være invertibel. Derfor er $A$ invertibel. 

(b) Betragt matricen $[A \quad I_n]$. Enhver $n \times n$ matrix, $A$, kan skrives som en matrix, $R$, på reduceret trappeform, ved hjælp af elementære rækkeoperationer. Udføres de samme rækkeoperationer på $n \times 2n$ matricen, $[A \quad I_n]$, tranformeres denne til $[R \quad B]$. Betragt en invertibel matrix $P$, som opfyldet at: $P[A \quad I_n]=[R \quad B]$. Så følger det at:  
\begin{align*}
[R \quad B]=P[A \quad I_n]=[PA \quad PI_n]=[PA \quad P]. 
\end{align*}
Så er $PA=R$ og $P=B$. Hvis $R \neq I_n$ så er $A$ ikke invertibel. Derfor må $R=I_n$, og dermed er $PA=I_n\Leftrightarrow P=A^{-1}$. P er da lig med både $A^{-1}$ og $B$, og $A^{-1}=B$. 
$$[A \quad I_n]\sim[R \quad B]=[PA \quad P]=[I_n \quad A^{-1}]$$
\end{proof}

\begin{eks}
Givet en matrix A ønskes det nu at finde en inverse. 
\begin{align*}
A= \begin{bmatrix}
1 & 3 \\
1 & 2
\end{bmatrix}
\end{align*}
Først opstilles $[A \quad I_n]$ matricen: 
\begin{align*}
\begin{bmatrix}
1 & 3 & 1 & 0 \\
1 & 2 & 0 & 1
\end{bmatrix}.
\end{align*}
Nu udførers elementære rækkeoperationer således at $A$ transformeres til $I_n$. 
\begin{align*}
\begin{bmatrix}
1 & 3 & 1 & 0 \\
1 & 2 & 0 & 1
\end{bmatrix}.
\sim \begin{bmatrix}
1 & 3 & 1 & 0 \\
0 & -1 & -1 & 1
\end{bmatrix}.
\sim \begin{bmatrix}
1 & 3 & 1 & 0 \\
0 & 1 & 1 & -1
\end{bmatrix}.
\sim \begin{bmatrix}
1 & 0 & -2 & 3 \\
0 & 1 & 1 & -1
\end{bmatrix}.
\end{align*}
Det er nu muligt at aflæse $A^{-1}$:
\begin{align*}
A^{-1} =\begin{bmatrix}
-2 & 3\\
1 & -1 
\end{bmatrix}.
\end{align*}
\end{eks}

\subsection{Elementærmatricer og invertibilitet}
Generelt kaldes en $n \times n$ matrix $E$ en elementærmatrix, hvis der på identitetsmatricen $I_n$ kan udføres én elementær rækkeoperation, så der fås $E$. 

\begin{eks}
En matrix $E$ er givet ved
\begin{align*}
E =\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 2 \\
0 & 0 & 1 
\end{bmatrix}.
\end{align*}
Her er der lagt to gange række $3$ til række $2$ på identitetsmatricen $I_3$ for at få $E$.

Det ses også, at givet følgende matrix
\begin{align*}
A =\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 
\end{bmatrix}
\end{align*}
så er
\begin{align*}
EA =\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 2 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6
\end{bmatrix}
=\begin{bmatrix}
1 & 2 \\
13 & 16 \\
5 & 6
\end{bmatrix}.
\end{align*}
Det ses altså, at den rækkeoperation, der blev udført på $I_3$ for at få $E$, er samme rækkeoperation, der kan udføres på $A$ for at få $EA$. 
\end{eks}

\begin{defn}
Enhver elementærmatrix er invertibel. Den inverse af en elementærmatrix er også en elementærmatrix. 
\end{defn}

Enhver rækkeoperation kan laves omvendt. For eksempel kan den matrix der fås, ved at lægge to gange den første række til anden række, ændres tilbage ved at tage $-2$ gange den første række og lægge til den anden række i den nye matrix. På den måde opnås den inverse af en elementærmatrix. \\
Ud fra et antal elementære rækkeoperationer kan der opnås en matrix på reduceret trappeform. Dette kan ligeledes opnås, ved at gange samme antal elementærmatricer, som elementære rækkeroperationer, på den givne matrix. 
Hvis $A$ er en $m \times n$ matrix på reduceret trappeform $R$, findes der elementærmatricer $E_1$, $E_2$, \dots, $E_k$, så 
\begin{align*}
E_k E_{k-1} \dotsm E_1 A = R.
\end{align*}

Hvis $P = E_k E_{k-1} \dotsm E_1$, så er $P$ et produkt af elementærmatricer og derfor invertibel. Derudover er $PA = R$. 

\begin{stn}\label{stn:mn_invertibel}
Lad $A$ være en $m \times n$ matrix på reduceret trappeform $R$. Da findes der en invertibel $m \times m$ matrix $P$ så $PA=R$. 
\end{stn}

Sætning \ref{stn:mn_invertibel} følger af følgende resultat. 

\begin{kor}
Matrix-ligningen $A\vec{x}=\vec{b}$ har samme løsninger som $R\vec{x}=\vec{c}$ hvor $\begin{bmatrix} R & \vec{c} \end{bmatrix}$ er matricen $\begin{bmatrix} A & \vec{b} \end{bmatrix}$ på reduceret trappeform.
\end{kor}

\begin{proof}
Der findes en invertibel matrix $P$ således at $P \begin{bmatrix} A & \vec{b} \end{bmatrix} = \begin{bmatrix} R & \vec{c} \end{bmatrix}$. Derfor
\begin{align*}
\begin{bmatrix} 
PA & P\vec{b} 
\end{bmatrix} =
P \begin{bmatrix}
A & \vec{b}
\end{bmatrix} =
\begin{bmatrix}
R \vec{c}
\end{bmatrix},
\end{align*}
og altså $PA = R$ og $P\vec{b}=\vec{c}$. Da $P$ er invertibel, følger det at $A = P^{-1}R$ og $\vec{b}=P^{-1}\vec{c}$.
Hvis $\vec{v}$ er en løsning til $A\vec{x}=\vec{b}$, så $A\vec{v}=\vec{b}$. Så
\begin{align*}
R\vec{v} = (PA)\vec{v} = P(A\vec{v}) = P\vec{b}=\vec{c}. 
\end{align*} 
Hvorfor $\vec{v}$ er en løsning til $R\vec{x}=\vec{c}$. Antag at $\vec{v}$ er en løsning til $R\vec{x} = \vec{c}$. Så er $R\vec{v} = \vec{c}$, og derfor
\begin{align*}
A\vec{v} = (P^{-1}R)\vec{v} = P^{-1}R\vec{v} = P^{-1}\vec{c} = \vec{b}. 
\end{align*}
Derfor er $\vec{v}$ en løsning til $A\vec{x} = \vec{b}$. Udtrykkene $A\vec{x} = \vec{b}$ og $R\vec{x} = \vec{c}$ har altså samme løsninger.
\end{proof}

\input{incl/main/Lin_alg/Determinant}
\input{incl/main/Lin_alg/undspanbas}
\input{incl/main/Lin_alg/ortogonalitet}