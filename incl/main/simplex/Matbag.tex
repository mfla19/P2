\begin{stn}
Lad $\vec{x} \in P =\{\vec{x}\in \mathds{R}^n \mid A\vec{x}= \vec{b}, \vec{x}\geq \vec{0}\}$ være en basisløsning, da eksistere der en vektor $\vec{d}\in \mathds{R}^n$, og en skalar $\theta^*\geq 0$ så $\vec{y}= \vec{x} + \theta^* \vec{d}$ er en nabo basisløsning til $\vec{x}$, $P$ ikke indeholder en linje.
\label{stn:matbagsim}
\end{stn}
Sætning \ref{stn:matbagsim} består af tre påstande, at der eksistere en vektor $\vec{d}$, en skalar $\theta^*$ og at $\vec{y}$ er en basisløsning.
Derfor splittes sætningen op i lemmaer.
\begin{lma}
Lad $\vec{x} \in P =\{\vec{x}\in \mathds{R}^n \mid A\vec{x}= \vec{b}, \vec{x}\geq \vec{0}\}$ være en basisløsning, da eksistere der en vektor $\vec{d}\in \mathds{R}^n$, så $\vec{x} + \vec{d} \in P$, og så $\vec{d}$ introducere $x_j$ og kun $x_j$ til basisløsningen.
\label{lma:retningsvektor}
\end{lma}
\begin{proof}
%Først konstrueres $\vec{d}$.
Lad $B$ betegne basismatrixen til $\vec{x}$, så $\vec{A_{B(i)}}$ er en søjle i $B$ for et index $I_B =\{B(1),...,B(m)\}$. 
%Da $\vec{y}$ skal være en nabo basisløsning, må der eksistere en variable $x_j$ for $j \notin I_B$, så $x_j = 0$ for $\vec{x}$ og $\vec{x_j} \neq 0 $ for $\vec{y}$. 
Da skal $\vec{d}$ introducere $x_j$ og kun $x_j$ til basisløsningen, konstruer derfor $\vec{d}$ så $x_j \neq 0$ for $\vec{x}+\vec{d}$, lad derfor $d_j=1$, og $d_i = 0$ for $i\notin I_B$ og $i \neq j$. 
Bemærk at da $\vec{x}+ \theta^*\vec{d} \in P$ må
\begin{align*}
	A(\vec{x}+\theta^* \vec{d}) = A\vec{x} + \theta^* A \vec{d}  & = \vec{b} \qquad \Rightarrow
	\\ A \vec{d} &= \vec{0}
\end{align*}
Lad nu $\vec{d}_B$ betegne indgangene i $\vec{d}$ så $d_i \in \vec{d}_B$ for $i \in I_B$. 
Det følger derfor, at
\begin{align*}
A \vec{d} = \sum_{i \in I_B} \vec{A_i} \vec{d_i} + \vec{A_j} &= B\vec{d}_B + \vec{A_j} = 0 \qquad \Longrightarrow
\\ \vec{d}_B &= -B^{-1}\vec{A_j}.
\end{align*}
Bemærk, at $B^{-1}$ eksistere ifølge Sætning %Mangler.
Det betyder at $\vec{d}_B = -B^{-1}\vec{A_j}$, mens at $d_j = 1$ og $d_i = 0$ for $i\notin I_B$ og $i \neq j$.
\end{proof}

\begin{defn}[$j$te retnings vektor]
Lad $\vec{x} \in P =\{\vec{x}\in \mathds{R}^n \mid A\vec{x}= \vec{b}, \vec{x}\geq \vec{0}\}$ være en basisløsning med basis matrix $B$, så $\vec{A_{B(i)}}$ er en søjle i $B$ for et index $I_B =\{B(1),...,B(m)\}$, da er den \textbf{$j$te retnings vektor} givet ved $\vec{d}_B = - B^{-1} A_j$ $d_j = 1$ og $d_i = 0$ for $i\notin I_B$ og $i \neq j$.
\end{defn}

\begin{lma}
Lad $\vec{x} \in P =\{\vec{x}\in \mathds{R}^n \mid A\vec{x}= \vec{b}, \vec{x}\geq \vec{0}\}$ være en basisløsning, $\vec{d}$ være den $j$te retningsvektor.
Da eksistere der en skalar $\theta^* \geq 0$, så $\vec{x} + \theta^* \vec{d} \in P$, og så $\theta^* $ fjerne $x_l$ fra basisløsningen, hvis $P$ ikke indeholder en halvlinje.
\label{lma:skalar}
\end{lma}

\begin{proof}
Lad $B$ betegne basismatrixen til $\vec{x}$, så $\vec{A_{B(i)}}$ er en søjle i $B$ for et index $I_B =\{B(1),...,B(m)\}$. 
Antag først at $\vec{d}\geq \vec{0}$ da vil $\vec{x}+\theta^*\vec{d}$ altid overholde alle lighedsbetingelser og positivitets betingelser for $\theta^* \geq 0$. 
Derfor vil $P$ indeholde en linje, da det går mod antagelsen om at $P$ ikke indeholder en linje, må mindst en indgang i $\vec{d}$ være negativ, kald denne for $d_i$.
Da må 
\begin{align*}
 0 &= x_i + \theta^* d_i \qquad \Rightarrow
 \\ \theta^* = -\frac{d_i}{x_i}.
\end{align*} 
\\Hvis $\vec{x} + \theta^* \vec{d} \in P$, må intet $x_i$ blive mindre end $0$, hvorfor $x_l$ vælges så $\frac{x_i}{d_i}$ er så lille som muligt for $d_i < 0$. 
Dermed bliver $\theta^* = \underset{i \in I_B}{\min}\{\frac{x_i}{d_i}\}$. 
\end{proof}

\begin{defn}[$j$te skalar]
Lad $\vec{x} \in P =\{\vec{x}\in \mathds{R}^n \mid A\vec{x}= \vec{b}, \vec{x}\geq \vec{0}\}$ være en basisløsning, $\vec{d}$ være den $j$te retningsvektor, og $P$ ikke indeholder en linje.
Da er den \textbf{$j$te skalar} givet ved $\theta^* = \underset{i \in I_B}{\min}\{\frac{x_i}{d_i}\}$.
\end{defn}

\begin{proof}
Det følger af Lemma \ref{lma:retningsvektor} og Lemma \ref{lma:skalar}, at $\vec{d}$ og $\theta^*$ eksistere. 
For at vise at $\vec{y}$ er en basisløsning, bemærkes at $x_i = 0$ for $i \notin I_{B'} = I_B\setminus\{l\}\cup\{j\}$ og at Basismatrixen for $\vec{y}$ er matricen $B'$ med søjlerne $A_i$ for $i \in I_{B'}$. 
Det følger derfor af Sætning
at $\vec{y}$ er en basisløsning hvis søjlerne i $B'$ er lineært uafhængig.
Antag derfor for modstrid at søjlerne er lineært afhængige, da følger det af Definition
at
\begin{align*}
 \sum_{i \in I_{B'}} \lambda_i A_i = \vec{0}.
\end{align*}
Da $B$ og $B'$ kun er forskellige i en søjle, må
\begin{align*}
 \\ \sum_{i \in I_{B'}}  B^{-1} \lambda_i A_i  =\sum_{i \in I_{B'}\setminus \{j\}} \vec{e_i} + B^{-1} \lambda_j A_j = \vec{0}.
\end{align*}
Dvs. at søjlerne i $B'$ er lineært uafhængige, hvis $A_{jl} = 0$.
Bemærk at $B^{-1} \lambda_j A_j = - \vec{d}_B$, hvis $l$te indgang pr. definition er forskellig fra $0$. 
Hvorfor alle søjler $B'$ er lineært uafhængige, hvorfor at alle rækker er lineært uafhængige og $\vec{y}$ er en basisløsning.
Bemærk at da $\vec{x}\in P$ og $\vec{d}$ og $\theta^*$ er konstrueret så $\vec{x}+\theta^*\vec{d} \in P$ må $\vec{y} \in P$, hvormed $\vec{y}$ er en mulig basisløsning.
\end{proof}

\begin{stn}
Lad $\vec{x}$ være en basisløsning, med basismatrix $B$, da er ændringen i obejktfunktionen ved at introducere $x_j$ til løsningen givet ved
\begin{align*}
 \Delta c_j = c_j-\vec{c}_B B^{-1}\vec{A_j}.
\end{align*}
\end{stn}

\begin{proof}
Antag først at $x_j$ ikke er en basis variable til at starte med, da vil den $j$te retningsvektor introducere $x_j$ til løsningen, hvorfor,
\begin{align*}
\Delta c_j = \vec{c}^T(\vec{x}+ \vec{d}) - \vec{c}^T\vec{x} = \vec{c}^T\vec{d} = \sum_{i \in I_B} c_i d_i + c_j.
\end{align*}
Lad nu $\vec{c}_B = \rvect{c_{B(1)}& \cdots & c_{B(m)}}$, da vil
\begin{align}
\Delta c_j =\vec{c}_B\vec{d}_B+ c_j = c_j-\vec{c}_B B^{-1}\vec{A_j}.
\end{align}
Antag nu at $x_j$ er en basisvariable da vil 
\begin{align*}
\Delta c_j = \vec{c}^T\vec{x}- \vec{c}^T\vec{x} = 0.
\end{align*}
Det undersøges derfor om $ c_j-\vec{c}_B B^{-1}\vec{A_j}= 0$, hvis $x_j$ er en basisvariable.
\begin{align*}
 \Delta c_j = c_j-\vec{c}_B^T B^{-1}\vec{A_j} = c_j - \vec{c}_B^T \vec{e_j} = c_j - c_j = 0.
\end{align*}
Det kan derfor konkluderes at $\Delta c_j = c_j-\vec{c}_B B^{-1}\vec{A_j}$ for et hvert $x$.
\end{proof}

\begin{stn}
Lad $\vec{x}$ være en basisløsning, med basismatrix $B$, så er $\vec{x}$ optimal, hvis og kun hvis $\Delta c_j \geq 0$ for alle $j$.
\end{stn}.

\begin{proof}
Antag først at $\vec{x}$ er optimal, da vil $\vec{c}^T\vec{x} \leq \vec{c}^T\vec{y}$ for alle $\vec{y} \in P$. 
Det følger af Sætning
at den optimale løsning er en basisløsning, derfor kan det antages at $\vec{y}$ er en basisløsning, derfor må der eksistere et $\theta^*$ og en $\vec{d}$ så $\vec{y} = \vec{x}+ \theta^* \vec{d}$.
Derfor må
\begin{align*}
	\vec{c}^T\vec{x}\leq \vec{c}^T\vec{y} = \vec{c}^T(\vec{x}+\theta^*\vec{d}) = \vec{c} + \theta^* \Delta c_j.
\end{align*}
Da $\theta^* \geq 0$ betyder det, at så længe $\Delta c_j \geq 0$ så er uligheden overholdt og $\vec{x}$ er optimal.
\\ Antag nu at $\Delta c_j \geq 0$, da vil  
\begin{align*}
	\vec{c}^T\vec{x}\leq \vec{c}^T\vec{y} = \vec{c}^T(\vec{x}+\theta^*\vec{d}) = \vec{c} + \theta^* \Delta c_j.
\end{align*}
for et hvert $\vec{y} \in P$, hvorfor $\vec{x}$ er optimal.

\end{proof}




